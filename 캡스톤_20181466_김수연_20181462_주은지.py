# -*- coding: utf-8 -*-
"""unet1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NcusNb99FRYKM4riE46OrKgqZjlBSrF8
"""

import numpy as np # linear algebra
import pandas as pd
import os
for dirname, _, filenames in os.walk('/content/drive/MyDrive/UNEt'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
        break

import matplotlib.pyplot as plt
import keras
from tensorflow.keras import Sequential 
from tensorflow.keras.utils import Sequence, to_categorical, plot_model
from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator
from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Input, MaxPooling2D, concatenate, BatchNormalization, Activation, Dropout
from tensorflow.keras.models import load_model, Model
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from random import sample, choice
from PIL import Image

import warnings
warnings.filterwarnings("ignore")

'''디렉토리에서 데이터 불러와서 확인하기'''

original_img_lst = os.listdir("/content/drive/MyDrive/UNEt/Original_data")
labeled_img_lst = os.listdir("/content/drive/MyDrive/UNEt/Labeled_data")
print(len(original_img_lst), len(labeled_img_lst))  #데이터가 제대로 들어왔는지 확인
print(type(original_img_lst[0].split('.')[0]))

#original_img_lst

type(original_img_lst) #불러온 데이터는 list형태로 들어옴.



'''원본 이미지와 mask 이미지를 쌍으로 저장하는 함수 생성. 원본 이미지와 원본 이미지에 mask한 이미지를 제대로 연결해야하므로 img_lst에는 원본 이미지 이름을 넣고 image_dir, mask_dir에는 해당 이미지를 가지고 있는 경로를 넣음. '''

def make_pair(img_lst,image_dir,mask_dir):  
    pairs = [] #image pair를 저장할 리스트.
    
    #모든 원본 이미지와 mask된 이미지를 쌍으로 묶어 리스트에 추가함. 
    for im in img_lst:  
        pairs.append((image_dir + im, mask_dir + im.split('.')[0]+'_L.png'))  #mask된 이미지의 경우 파일 끝에 "_L.png"를 붙임. 
        
    return pairs  #해당 리스트를 반환.

'''앞서 만든 make_pair 함수를 사용하여 원본 이미지와 mask 이미지를 쌍으로 저장.'''
data_pairs = make_pair(original_img_lst, "/content/drive/MyDrive/UNEt/Original_data/" ,
                      "/content/drive/MyDrive/UNEt/Labeled_data/")
data_pairs[0]  #함수가 제대로 실행되었는지 확인.

'''만들어진 이미지쌍이 list 형태로 저장되었는지 확인.'''
type(data_pairs)

#data_pairs  #전체 데이터 확인.



'''random.shuffle. 모델을 학습하기 전에 모델이 데이터를 랜덤한 순서로 학습할 수 있도록 데이터 페어의 순서를 랜덤하게 섞음.''' 
import random
random.shuffle(data_pairs)

#data_pairs #데이터가 섞였는지 확인.

'''모델에 데이터를 적용하기 위해서 데이터를 train data, validation data, test data를 각각 70%, 10%, 20%으로 나눔.'''
train_pairs = data_pairs[0:490]  #70%
val_pairs = data_pairs[490:580]  #10%
test_pairs = data_pairs[580:701]  #20%



'''데이터가 잘 페어 되었는지 디렉토리에서 데이터를 무작위로 선택하여 이미지를 확인할 수 있음. '''
temp = choice(train_pairs) #train_pairs에서 하나의 페어를 랜덤으로 선택. 

#선택된 페어에서 오리지널 이미지, mask된 이미지를 불러와 각각 img, mask에 저장
img = img_to_array(load_img(temp[0]))  
mask = img_to_array(load_img(temp[1]))
#mask_pil = np.asarray(Image.open(temp[1]))

#이미지를 plt으로 띄움.
plt.figure(figsize=(12,12))
plt.subplot(121)
plt.title("Image")
plt.imshow(img/255)
plt.subplot(122)
plt.title("Mask")
plt.imshow(mask/255)
#plt.subplot(123)
#plt.imshow(mask_pil)
plt.show()



'''제공된 class map 파일을 사용해 mask된 이미지에서 각 class가 어떤것을 나타내는지 연결시킴. 이를 로드하여 매핑의 작동방식을 확인.'''

class_map_df = pd.read_csv("/content/drive/MyDrive/UNEt/class_dict2.csv")
class_map_df

'''매핑된 데이터를 사용하여 해당 class가 어떤 색으로 표현되는지 class_map에 넣음.'''
class_map = []  
for index,item in class_map_df.iterrows():  #iterrows()를 사용하여 첫번째 변수인 "name"을 index로 받고 나머지 r,g,b를 해당 index에 대한 item에 넣음. 
    class_map.append(np.array([item['r'], item['g'], item['b']]))  #iterrows()로 정리한 item들을 np.array형태로 만들어 리스트에 추가함.
    
print(len(class_map))
print(class_map[0])

"""This function will be used later, to assert that mask should contains values that are class labels only.
   Like, our example has 32 classes , so predicted mask must contains values between 0 to 31. 
   So that it can be mapped to corresponding RGB."""
#해당 픽셀이 어떤 class에 속하는지 판단할 수 있도록 RGB를 매핑해줌.
def assert_map_range(mask,class_map):
    mask = mask.astype("uint8") #mask를 전부 "uint8"로 변환
    for j in range(img_size):
        for k in range(img_size):
            assert mask[j][k] in class_map , tuple(mask[j][k])

'''This method will convert mask labels(to be trained) from RGB to a 2D image whic holds class labels of the pixels.'''
'''RGB에서 픽셀의 class label을 가지는 2D 이미지로 mask label을 변환하는 함수'''
def form_2D_label(mask,class_map):
    mask = mask.astype("uint8")  #mask를 "uint8"로 변환. 
    label = np.zeros(mask.shape[:2],dtype= np.uint8) #차원 항목을 빼 2차원으로 변형한 mask의 크기로 0행렬을 만듬. 즉, 이미지의 각 픽셀에 해당하는 행렬을 만듬.
    
    for i, rgb in enumerate(class_map):  #class_map에서 클래스와 해당 클래스에 대한 rgb값을 가져와 각각 i, rgb에 넣고
        label[(mask == rgb).all(axis=2)] = i  #mask와 같은 rgb를 가지는 클래스를 해당 픽셀의 위치에 넣어줌,
    
    return label

lab = form_2D_label(mask,class_map)  #해당 이미지에서 어떤 class를 가지고 있는지 확인.
np.unique(lab,return_counts=True) #해당 이미지에서 특정 class의 픽셀이 몇개나 포함되어 있는지 count.



'''custom data generator'''
class DataGenerator(Sequence):
    'Keras를 사용하여 데이터 생성' 
    
    def __init__(self, pair,class_map,  batch_size=16, dim=(224,224,3), shuffle=True):
        '초기화'
        self.dim = dim   #data size
        self.pair = pair  #데이터 페어
        self.class_map = class_map  #class
        self.batch_size = batch_size   #batch size
        self.shuffle = shuffle  #shuffle을 허용. 
        self.on_epoch_end()  #훈련 중 epoch가 끝날 때 호출.

    def __len__(self):
        '한 epoch 당 batch의 개수를 나타내는 함수'
        return int(np.floor(len(self.pair) / self.batch_size))

    def __getitem__(self, index):
        'Generate one batch of data'
        'batch 하나 생성.'
        #batch의 index 생성.
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]

        #IDs의 list 찾기
        list_IDs_temp = [k for k in indexes]

        #데이터 생성
        X, y = self.__data_generation(list_IDs_temp)
        return X, y

    def on_epoch_end(self):
        '각 epoch가 끝난 후에 index를 업데이트. index 순서를 무작위로 섞어줌. '
        self.indexes = np.arange(len(self.pair))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, list_IDs_temp):
        'batch_size samples을 포함하는 데이터 생성' # X : (n_samples, *dim, n_channels)
        # 초기화
        batch_imgs = list()  #배치한 이미지정보를 배열로 저장
        batch_labels = list()  #각 클래스가 포함된 이미지 정보를 배열로 저장. 

        # 데이터 생성
        for i in list_IDs_temp:
            # Store sample
            img = load_img(self.pair[i][0] ,target_size=self.dim) #i번째 이미지 페어의 원본을 불러와 target_size로 이미지 로드. 
            img = img_to_array(img)/255.   #로드한 이미지를 numpy array로 변환하여 255로 나눔. 
            batch_imgs.append(img)  #리스트에 추가. 

            label = load_img(self.pair[i][1],target_size=self.dim)  #masked된 이미지도 원본 이미지와 같은 과정을 거침.
            label = img_to_array(label)
            #------ comment these two lines to see proper working of datagenerator in cell below----#
            label = form_2D_label(label,self.class_map)  #numpy array가 된 label의 각 픽셀이 해당하는 class를 찾음.
            label = np.asarray(to_categorical(label , num_classes = 32))  #이후 배열을 32개의 클래스를 가진 이진 배열로 만듬.
            #------ But after that uncomment again them before training the model----------#
            #------ comment them to just run the cell below and again uncomment these two lines----#
            #print(label.shape)
            batch_labels.append(label)
        return np.array(batch_imgs) ,np.array(batch_labels)

img_size = 512  #이미지 사이즈. 메모리 문제로 크기를 줄였음. 
#class_map = class_palette()

#DataGenerator의 기능을 확인
#DataGenerator의 객체를 만들고
train_generator1 = DataGenerator(train_pairs,class_map,batch_size=4, dim=(img_size,img_size,3) ,shuffle=True)
X,y = train_generator1.__getitem__(0) #original img, labeled img의 정보를 담고 있는 배열을 각각 X, y로 지정. 
print(X.shape, y.shape) #배열의 크기를 확인. 

#잘 저장되어 있는지 이미지로 확인. 
plt.figure(figsize=(12, 6))
print("Images")
for i in range(4):
    plt.subplot(2, 4, i+1)
    plt.imshow(X[i])
plt.show()

'''dim이 (img_size,img_size,3)인 컬러 이미지가 되도록 pair데이터를 사용하여 DataGenerator객체를 만듬.'''
train_generator = DataGenerator(train_pairs,class_map,batch_size=4, dim=(img_size,img_size,3) ,shuffle=True) #train_pairs를 이용해 만든 DataGenerator 객체.
val_generator = DataGenerator(val_pairs, class_map, batch_size=4, dim=(img_size,img_size,3) ,shuffle=True) #val_pairs를 이용해 만든 DataGenerator 객체.
test_generator = DataGenerator(test_pairs, class_map, batch_size=4, dim=(img_size,img_size,3) ,shuffle=True) #test_pairs를 이용해 만든 DataGenerator 객체.



'''U-Net Model
# keras tensor를 인자로 사용, 필터는 지정하지 않음. 컬러이므로 3채널 padding을 same으로 해서 대소문자를 무시하고 output과 input이 동일하도록 설정 karas 정규분포를 이용해서 초기화로 케인 he가 만든 ReLU
def conv_block(tensor, nfilters, size=3, padding='same', initializer="he_normal"):
    x = Conv2D(filters=nfilters, kernel_size=(size, size), padding=padding, kernel_initializer=initializer)(tensor)#컨볼루션 레이어 종류 많지만 여기선 영상처리에 쓰이는 conv2D 레이어
    x = BatchNormalization()(x)#배치 정규화
    x = Activation("relu")(x)#활성함수로 relu
    x = Conv2D(filters=nfilters, kernel_size=(size, size), padding=padding, kernel_initializer=initializer)(x)
    x = BatchNormalization()(x)
    x = Activation("relu")(x)
    return x

def deconv_block(tensor, residual, nfilters, size=3, padding='same', strides=(2, 2)): #conv_block과 동일하지만 strides를 2로 설정해서 2칸씩 이동하도록 함
    y = Conv2DTranspose(nfilters, kernel_size=(size, size), strides=strides, padding=padding)(tensor) # Conv2DTranspose 레이어는 keras에서 upsampling과 conv2D를 합친 함수 제공
    y = concatenate([y, residual], axis=3) # 출력을 연결해주기 위해서 사용
    y = conv_block(y, nfilters)
    return y
def Unet(h, w, filters, num_classes = 32):#32개 클래스수로 필터 고정
# Down
    input_layer = Input(shape=(h, w, 3), name='image_input') #input_shape=(img_height, img_width, nclasses))(output_layer)
    conv1 = conv_block(input_layer, nfilters=filters)
    conv1_out = MaxPooling2D(pool_size=(2, 2))(conv1)# 불필요한 정보를 간추리기 위해 pool_size를 2로 해서 절반으로 줄인다.
    conv2 = conv_block(conv1_out, nfilters=filters*2)
    conv2_out = MaxPooling2D(pool_size=(2, 2))(conv2)
    conv3 = conv_block(conv2_out, nfilters=filters*4)
    conv3_out = MaxPooling2D(pool_size=(2, 2))(conv3)
    conv4 = conv_block(conv3_out, nfilters=filters*8)
    conv4_out = MaxPooling2D(pool_size=(2, 2))(conv4)
    conv4_out = Dropout(0.5)(conv4_out)#과적합을 피하기 위해 은닉층 배치 노드중 일부 50% 끈다.
    conv5 = conv_block(conv4_out, nfilters=filters*16)
    conv5 = Dropout(0.5)(conv5)
# Up
    deconv6 = deconv_block(conv5, residual=conv4, nfilters=filters*8)
    deconv6 = Dropout(0.5)(deconv6)
    deconv7 = deconv_block(deconv6, residual=conv3, nfilters=filters*4)
    deconv7 = Dropout(0.5)(deconv7) 
    deconv8 = deconv_block(deconv7, residual=conv2, nfilters=filters*2)
    deconv9 = deconv_block(deconv8, residual=conv1, nfilters=filters)
    #output_layer = Reshape((img_height*img_width, nclasses) 
    output_layer = Conv2D(filters=num_classes, kernel_size=(1, 1), activation='softmax')(deconv9)#각 구성요소 피쳐의 벡터를 클래스 수에 매핑

    model = Model(inputs=input_layer, outputs=output_layer, name='Unet')
    return model
'''
# keras tensor를 인자로 사용, 필터는 지정하지 않음. 컬러이므로 3채널 padding을 same으로 해서 대소문자를 무시하고 output과 input이 동일하도록 설정 karas 정규분포를 이용해서 초기화로 케인 he가 만든 ReLU
def conv_block(tensor, nfilters, size=3, padding='same', initializer="he_normal"):
    x = Conv2D(filters=nfilters, kernel_size=(size, size), padding=padding, kernel_initializer=initializer)(tensor)#컨볼루션 레이어 종류 많지만 여기선 영상처리에 쓰이는 conv2D 레이어
    x = BatchNormalization()(x)#배치 정규화
    x = Activation("relu")(x)#활성함수로 relu
    x = Conv2D(filters=nfilters, kernel_size=(size, size), padding=padding, kernel_initializer=initializer)(x)
    x = BatchNormalization()(x)
    x = Activation("relu")(x)
    return x


def deconv_block(tensor, residual, nfilters, size=3, padding='same', strides=(2, 2)):#conv_block과 동일하지만 strides를 2로 설정해서 2칸씩 이동하도록 함
    y = Conv2DTranspose(nfilters, kernel_size=(size, size), strides=strides, padding=padding)(tensor)# Conv2DTranspose 레이어는 keras에서 upsampling과 conv2D를 합친 함수 제공
    y = concatenate([y, residual], axis=3)# 출력을 연결해주기 위해서 사용
    y = conv_block(y, nfilters)
    return y
    
def Unet(h, w, filters, num_classes = 32):#32개 클래스수로 필터 고정
# down
    input_layer = Input(shape=(h, w, 3), name='image_input')#input_shape=(img_height, img_width, nclasses))(output_layer)
    conv1 = conv_block(input_layer, nfilters=filters)
    conv1_out = MaxPooling2D(pool_size=(2, 2))(conv1)# 불필요한 정보를 간추리기 위해 pool_size를 2로 해서 절반으로 줄인다.
    conv2 = conv_block(conv1_out, nfilters=filters*2)
    conv2_out = MaxPooling2D(pool_size=(2, 2))(conv2)
    conv3 = conv_block(conv2_out, nfilters=filters*4)
    conv3_out = MaxPooling2D(pool_size=(2, 2))(conv3)
    conv4 = conv_block(conv3_out, nfilters=filters*8)
    conv4_out = MaxPooling2D(pool_size=(2, 2))(conv4)
    conv4_out = Dropout(0.5)(conv4_out)#과적합을 피하기 위해 은닉층 배치 노드중 일부 50% 끈다.
    conv5 = conv_block(conv4_out, nfilters=filters*16)
    conv5 = Dropout(0.5)(conv5)
    
    
    #NIN(1x1 convolution)을 사용하여 parameter를 줄여보자. 
    nin = Conv2D(filters=filters, kernel_size = (1,1), padding='valid', kernel_initializer="he_normal")(conv5)
    nin = BatchNormalization()(nin)
    nin = Activation("relu")(nin)
    nin = MaxPooling2D(pool_size=(1,1))(nin)
    nin = Conv2D(filters=filters*2, kernel_size=(3, 3), padding='same', kernel_initializer="he_normal")(nin)
    #nin
    nin = Conv2D(filters=filters, kernel_size = (1,1), padding='valid', kernel_initializer="he_normal")(conv5)
    nin = BatchNormalization()(nin)
    nin = Activation("relu")(nin)
    nin = Conv2D(filters=filters*2, kernel_size=(3, 3), padding='same', kernel_initializer="he_normal")(nin)
    #nin
    nin = Conv2D(filters=filters, kernel_size = (1,1), padding='valid', kernel_initializer="he_normal")(conv5)
    nin = BatchNormalization()(nin)
    nin = Activation("relu")(nin)
    nin = MaxPooling2D(pool_size=(1,1))(nin)
    nin = Conv2D(filters=filters*2, kernel_size=(3, 3), padding='same', kernel_initializer="he_normal")(nin)
    

# up
    deconv6 = deconv_block(nin, residual=conv4, nfilters=filters*8)
    deconv6 = Dropout(0.5)(deconv6)
    deconv7 = deconv_block(deconv6, residual=conv3, nfilters=filters*4)
    deconv7 = Dropout(0.5)(deconv7) 
    deconv8 = deconv_block(deconv7, residual=conv2, nfilters=filters*2)
    deconv9 = deconv_block(deconv8, residual=conv1, nfilters=filters)
    #output_layer = Reshape((img_height*img_width, nclasses) 
    output_layer = Conv2D(filters=num_classes, kernel_size=(1, 1), activation='softmax')(deconv9)#각 구성요소 피쳐의 벡터를 클래스 수에 매핑

    model = Model(inputs=input_layer, outputs=output_layer, name='Unet')
    return model

model = Unet(img_size , img_size , 64)
model.summary()

class MeanIoU(object):
    """
    MeanIoU 평균값으로 일반적으로 semantic segmentation 평가지표. 예측은 confusion matrix에 누적
    IoU는 IoU = true_positive / (true_positive + false_positive + false_negative) 계산
    평균 IoU는 모든 클래스간 IoU 평균임 
    """

    def __init__(self, num_classes):
        super().__init__()

        self.num_classes = num_classes

    def mean_iou(self, y_true, y_pred):
        """모델에 전달할 metric함수.
        Args:
            y_true (tensor): 맞는 라벨.
            y_pred (tensor): y_true과 동일한 예측
        numpy arrays를 인수로 사용해서 output으로 반환
        """
        return tf.py_function(self._mean_iou, [y_true, y_pred], tf.float32)

    def _mean_iou(self, y_true, y_pred):
        """
        numpy를 사용해서 MeanIoU 계산 confusion matrix를 계산해서 matrix 값을 가져옴
        예측 및 대상을 범주에서 정수로 변환
        """
        target = np.argmax(y_true, axis=-1).ravel()
        predicted = np.argmax(y_pred, axis=-1).ravel()

        # 2 arrays 함께 계산 해줌
        x = predicted + self.num_classes * target
        bincount_2d = np.bincount(
            x.astype(np.int32), minlength=self.num_classes**2
        )
        assert bincount_2d.size == self.num_classes**2
        conf = bincount_2d.reshape(
            (self.num_classes, self.num_classes)
        )

        # confusion matrix에서 IoU와 MeanIoU 계산
        true_positive = np.diag(conf)
        false_positive = np.sum(conf, 0) - true_positive
        false_negative = np.sum(conf, 1) - true_positive

        # 0으로 나눠질 경우 해당 클래스에서 0픽셀을 예측하고 배치에 동일 클래스에 0픽셀 있으므로 오류를 무시하고 값을 1로 설정
        with np.errstate(divide='ignore', invalid='ignore'):
            iou = true_positive / (true_positive + false_positive + false_negative)
        iou[np.isnan(iou)] = 1

        return np.mean(iou).astype(np.float32)

miou_metric = MeanIoU(32)

import tensorflow as tf
#keras.optimizers.adam을 사용하고 learning_rate를 1e-03. 클래스가 3개 이상으므로 categorical_crossentropy
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-03), loss='categorical_crossentropy' ,metrics=[miou_metric.mean_iou])

#mode를 auto로 지정해서 알아서 선택하게 함 save_best_only 옵션으로 monitor 되는 값이 좋은 값을 저장
mc = ModelCheckpoint(mode='auto', filepath='top-weights.h5', monitor='val_mean_iou',save_best_only='True', verbose=1)
#개선 여지 없을시 val_acc 기준으로 종료. patience는 epochs 10개 개선되지 않으면 종료
es = EarlyStopping(monitor='val_acc', patience=10, verbose=0)

#원래 있던 마스크와 예측한 마스크를 비교
train_steps = train_generator.__len__()
val_steps = val_generator.__len__()

print(train_steps, val_steps)

#모델을 학습시키고 generator 이미지 담은 배치로 학습 시키기 위해 fit_generator함수를 사용. 
results = model.fit_generator(train_generator , steps_per_epoch=train_steps ,epochs=80,
                              validation_data=val_generator,validation_steps=val_steps,callbacks=[mc,es],verbose = 1)

#model.save('/content/drive/MyDrive/UNEt/output/camvid_unet_model.h5')

'''load the saved model to evaluate it'''
#trained_model = keras.models.load_model("/content/drive/MyDrive/UNEt/output/camvid_unet_model.h5")

#test 데이터를 제공된 함수를 이용해서 평가합니다. 
test_score=model.evaluate_generator(test_generator, verbose=1)

print('Test Miou : ',test_score[1]*100)

x_test, y_test = test_generator.__getitem__(2)
print(x_test.shape, y_test.shape)

y_pred = model.predict(x_test, verbose = 1, batch_size = 4)
y_pred.shape

#예측하는 map이 RPG로 레이블 변환함
def map_this(y_pred,class_map):
    y_pred_rgb = np.zeros((y_pred.shape[0],y_pred.shape[1],y_pred.shape[2],3))
    for i in range(y_pred.shape[0]):
        image = np.zeros((y_pred.shape[1],y_pred.shape[2],3))
        for j in range(y_pred.shape[1]):
            for k in range(y_pred.shape[2]):
                image[j,k,:] = class_map[y_pred[i][j][k]]
        y_pred_rgb[i] = image
    return y_pred_rgb

#원본 이미지와 마스크된 이미지 예측한 이미지가 표시된다.
def x_plot_result(img , title):
    plt.figure(figsize=(12, 6))
    plt.title(title)
    for i in range(4):
        #print(pred[i].shape)
        plt.subplot(2, 4, i+1)
        plt.imshow(img[i])
    plt.show()

#원본 이미지와 마스크된 이미지 예측한 이미지가 표시된다.
def y_plot_result(img , title):
    plt.figure(figsize=(12, 6))
    plt.title(title)
    for i in range(4):
        #print(pred[i].shape)
        plt.subplot(2, 4, i+1)
        plt.imshow(img[i].astype('uint8'))
    plt.show()

pred = np.argmax(y_pred, axis=3)
y_pred_rgb = map_this(pred,class_map)
test = np.argmax(y_test, axis=3)
y_test_rgb = map_this(test,class_map)

x_plot_result(x_test,"Test Images")

y_plot_result(y_test_rgb,"Original Masks")

y_plot_result(y_pred_rgb,"Predicted mask")

